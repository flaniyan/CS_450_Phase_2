name: CD

on:
  push:
    branches: [main, performance_track]
  workflow_dispatch:

permissions:
  id-token: write
  contents: read
  actions: read
  pull-requests: read

concurrency:
  group: cd-main
  cancel-in-progress: false

env:
  TF_DIR: infra/envs/dev
  AWS_REGION: us-east-1
  ECR_REGISTRY: 838693051036.dkr.ecr.us-east-1.amazonaws.com
  ECR_REPOSITORY: validator-service
  ECS_SERVICE: validator-service
  ECS_CLUSTER: validator-cluster

jobs:
  deploy:
    runs-on: ubuntu-latest
    timeout-minutes: 25
    env:
      TF_IN_AUTOMATION: "true"
      TF_INPUT: "false"
      USE_OIDC: "true" # Set to "true" to use OIDC authentication
    permissions:
      id-token: write
      contents: read

    steps:
      - name: Checkout
        uses: actions/checkout@08eba0b27e820071cde6df949e0beb9ba4906955

      - name: Ensure Terraform plugin cache dir exists
        run: mkdir -p $HOME/.terraform.d/plugin-cache

      - name: Cache Terraform plugins
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830
        with:
          path: $HOME/.terraform.d/plugin-cache
          key: ${{ runner.os }}-tf-${{ hashFiles('**/*.tf') }}
          restore-keys: |
            ${{ runner.os }}-tf-

      - name: Debug OIDC Configuration
        if: ${{ env.USE_OIDC == 'true' }}
        run: |
          echo "USE_OIDC: ${{ env.USE_OIDC }}"
          echo "AWS_ROLE_TO_ASSUME secret exists: ${{ secrets.AWS_ROLE_TO_ASSUME != '' }}"
          echo "AWS_REGION: ${{ env.AWS_REGION }}"

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-region: ${{ env.AWS_REGION }}
          role-to-assume: arn:aws:iam::838693051036:role/github-actions-oidc-role
          role-session-name: GitHubActions-CDPipeline
      

      - name: Configure AWS credentials (Static)
        if: ${{ env.USE_OIDC != 'true' }}
        uses: aws-actions/configure-aws-credentials@ff717079ee2060e4bcee96c4779b553acc87447c
        with:
          aws-region: ${{ env.AWS_REGION }}
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      - name: Verify AWS identity
        shell: bash
        run: aws sts get-caller-identity

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build, tag, and push image to Amazon ECR
        id: build-image
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          # Build a docker container and push it to ECR
          docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
          docker tag $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG $ECR_REGISTRY/$ECR_REPOSITORY:latest
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:latest
          echo "image=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG" >> $GITHUB_OUTPUT
          echo "image_tag=$IMAGE_TAG" >> $GITHUB_OUTPUT

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@b9cd54a3c349d3f38e8881555d616ced269862dd
        with:
          terraform_wrapper: false

      - name: Ensure Terraform backend bucket exists
        shell: bash
        continue-on-error: true
        run: |
          BACKEND_BUCKET="pkg-artifacts"
          if ! aws s3api head-bucket --bucket "${BACKEND_BUCKET}" 2>/dev/null; then
            echo "Backend bucket ${BACKEND_BUCKET} does not exist. Creating it..."
            # us-east-1 doesn't use LocationConstraint
            if [ "${{ env.AWS_REGION }}" = "us-east-1" ]; then
              aws s3api create-bucket \
                --bucket "${BACKEND_BUCKET}" \
                --region "${{ env.AWS_REGION }}" || echo "Bucket may already exist or creation failed"
            else
              aws s3api create-bucket \
                --bucket "${BACKEND_BUCKET}" \
                --region "${{ env.AWS_REGION }}" \
                --create-bucket-configuration LocationConstraint="${{ env.AWS_REGION }}" || echo "Bucket may already exist or creation failed"
            fi
            
            # Enable versioning for the backend bucket
            aws s3api put-bucket-versioning \
              --bucket "${BACKEND_BUCKET}" \
              --versioning-configuration Status=Enabled || echo "Failed to enable versioning"
            
            # Enable encryption
            aws s3api put-bucket-encryption \
              --bucket "${BACKEND_BUCKET}" \
              --server-side-encryption-configuration '{"Rules":[{"ApplyServerSideEncryptionByDefault":{"SSEAlgorithm":"AES256"}}]}' || echo "Failed to enable encryption"
          else
            echo "Backend bucket ${BACKEND_BUCKET} already exists"
          fi

      - name: Ensure S3 access point exists
        shell: bash
        continue-on-error: true
        run: |
          ACCESS_POINT_NAME="cs450-s3"
          BUCKET_NAME="pkg-artifacts"
          ACCOUNT_ID="838693051036"
          
          # Check if access point exists
          if ! aws s3control get-access-point \
            --account-id "${ACCOUNT_ID}" \
            --name "${ACCESS_POINT_NAME}" \
            --region "${{ env.AWS_REGION }}" >/dev/null 2>&1; then
            echo "S3 access point ${ACCESS_POINT_NAME} does not exist. Creating it..."
            
            # Create access point
            aws s3control create-access-point \
              --account-id "${ACCOUNT_ID}" \
              --name "${ACCESS_POINT_NAME}" \
              --bucket "${BUCKET_NAME}" \
              --region "${{ env.AWS_REGION }}" \
              --public-access-block-configuration \
              "BlockPublicAcls=true,BlockPublicPolicy=true,IgnorePublicAcls=true,RestrictPublicBuckets=true" || \
            echo "Failed to create access point (may already exist or bucket doesn't exist)"
          else
            echo "S3 access point ${ACCESS_POINT_NAME} already exists"
          fi

      - name: Terraform init
        shell: bash
        working-directory: ${{ env.TF_DIR }}
        env:
          TF_PLUGIN_CACHE_DIR: $HOME/.terraform.d/plugin-cache
        run: |
          mkdir -p $HOME/.terraform.d/plugin-cache
          
          # Try normal init first
          if terraform init -input=false -lock-timeout=5m 2>&1 | tee /tmp/init-output.log; then
            echo "Terraform init succeeded"
          else
            # Check if it's a checksum mismatch error
            if grep -q "checksum" /tmp/init-output.log; then
              echo "Checksum mismatch detected. Attempting to fix..."
              
              # Wait for S3 eventual consistency
              echo "Waiting 20 seconds for S3 eventual consistency..."
              sleep 20
              
              # Try with -reconfigure to force re-initialization
              echo "Retrying with -reconfigure flag..."
              terraform init -reconfigure -input=false -lock-timeout=5m || {
                echo "Init still failed. You may need to manually fix the state checksum."
                echo "To fix manually, run:"
                echo "  aws dynamodb scan --table-name terraform-state-lock --region us-east-1"
                echo "  # Then delete any stale lock entries"
                exit 1
              }
            else
              echo "Terraform init failed with a different error"
              cat /tmp/init-output.log
              exit 1
            fi
          fi

      - name: Select or create workspace
        shell: bash
        working-directory: ${{ env.TF_DIR }}
        run: terraform workspace select default || terraform workspace new default

      - name: Terraform format check
        shell: bash
        working-directory: ${{ env.TF_DIR }}
        run: terraform fmt -check -recursive

      - name: Terraform validate
        shell: bash
        working-directory: ${{ env.TF_DIR }}
        run: terraform validate

      - name: Import existing DynamoDB tables
        shell: bash
        working-directory: ${{ env.TF_DIR }}
        continue-on-error: true
        env:
          TF_VAR_aws_region: us-east-1
          TF_VAR_artifacts_bucket: pkg-artifacts
        run: |
          # Import existing tables only if not already in state
          # Check if resource exists in state before importing
          import_table() {
            local resource_address=$1
            local table_name=$2
            if terraform state show "${resource_address}" >/dev/null 2>&1; then
              echo "Table ${table_name} already in state, skipping import"
            else
              echo "Importing ${table_name}..."
              terraform import -var="aws_region=us-east-1" -var="artifacts_bucket=pkg-artifacts" "${resource_address}" "${table_name}" || echo "Failed to import ${table_name} (may not exist or already imported)"
            fi
          }
          
          import_table 'module.ddb.aws_dynamodb_table.this["artifacts"]' artifacts
          import_table 'module.ddb.aws_dynamodb_table.this["users"]' users
          import_table 'module.ddb.aws_dynamodb_table.this["uploads"]' uploads
          import_table 'module.ddb.aws_dynamodb_table.this["tokens"]' tokens
          import_table 'module.ddb.aws_dynamodb_table.this["packages"]' packages
          import_table 'module.ddb.aws_dynamodb_table.this["downloads"]' downloads

      - name: Cleanup orphaned Terraform state resources
        shell: bash
        working-directory: ${{ env.TF_DIR }}
        continue-on-error: true
        run: |
          echo "Cleaning up orphaned resources from Terraform state..."
          
          # Function to safely remove from state
          remove_from_state() {
            local resource=$1
            if terraform state list | grep -q "^${resource}$"; then
              echo "Removing ${resource} from state..."
              terraform state rm "${resource}" || echo "Failed to remove ${resource} (may not exist)"
            else
              echo "${resource} not in state, skipping"
            fi
          }
          
          # Remove Config resources
          echo "Removing Config resources..."
          remove_from_state "module.config.aws_config_configuration_recorder.config_recorder"
          remove_from_state "module.config.aws_config_configuration_recorder_status.config_recorder"
          remove_from_state "module.config.aws_config_delivery_channel.config_delivery"
          remove_from_state "module.config.aws_iam_role.config_role"
          remove_from_state "module.config.aws_s3_bucket.config_snapshots"
          
          # Remove CloudTrail resources
          echo "Removing CloudTrail resources..."
          remove_from_state "module.monitoring.aws_cloudtrail.audit_trail"
          remove_from_state "module.monitoring.aws_s3_bucket.cloudtrail_logs"
          remove_from_state "module.monitoring.aws_s3_bucket_policy.cloudtrail_logs"
          
          echo "Cleanup complete"

      - name: Import S3 bucket if not in state
        shell: bash
        working-directory: ${{ env.TF_DIR }}
        continue-on-error: true
        env:
          TF_VAR_aws_region: us-east-1
          TF_VAR_artifacts_bucket: pkg-artifacts
        run: |
          # Import S3 bucket if it exists but is not in state
          if ! terraform state show module.s3.aws_s3_bucket.artifacts >/dev/null 2>&1; then
            echo "S3 bucket not in state, importing..."
            terraform import -var="aws_region=us-east-1" -var="artifacts_bucket=pkg-artifacts" module.s3.aws_s3_bucket.artifacts pkg-artifacts || echo "Failed to import bucket (may not exist)"
          else
            echo "S3 bucket already in state, skipping import"
          fi
          
          # Import encryption configuration if not in state
          if ! terraform state show module.s3.aws_s3_bucket_server_side_encryption_configuration.this >/dev/null 2>&1; then
            echo "S3 encryption configuration not in state, importing..."
            terraform import -var="aws_region=us-east-1" -var="artifacts_bucket=pkg-artifacts" module.s3.aws_s3_bucket_server_side_encryption_configuration.this pkg-artifacts || echo "Failed to import encryption config"
          else
            echo "S3 encryption configuration already in state, skipping import"
          fi

      - name: Terraform plan
        shell: bash
        working-directory: ${{ env.TF_DIR }}
        run: terraform plan -out=tfplan -input=false -lock-timeout=5m -var="aws_region=us-east-1" -var="artifacts_bucket=pkg-artifacts" -var="image_tag=${{ steps.build-image.outputs.image_tag }}"

      - name: Terraform apply
        shell: bash
        working-directory: ${{ env.TF_DIR }}
        run: terraform apply -auto-approve -lock-timeout=5m tfplan

      - name: Force ECS service update
        shell: bash
        run: |
          aws ecs update-service \
            --cluster $ECS_CLUSTER \
            --service $ECS_SERVICE \
            --force-new-deployment

      - name: Wait for ECS service to stabilize
        shell: bash
        run: |
          echo "Waiting for ECS service to stabilize..."
          echo "Checking service status before wait..."
          aws ecs describe-services \
            --cluster $ECS_CLUSTER \
            --services $ECS_SERVICE \
            --query 'services[0].{status:status,runningCount:runningCount,desiredCount:desiredCount,deployments:deployments[*].{id:id,status:status,runningCount:runningCount,desiredCount:desiredCount}}' \
            --output table
          
          echo "Starting wait for PRIMARY deployment to stabilize (max 15 minutes)..."
          MAX_ATTEMPTS=60
          ATTEMPT=0
          SLEEP_INTERVAL=15
          
          FAILED_ATTEMPTS=0
          MAX_FAILED_ATTEMPTS=5  # Fail after 5 consecutive failures (75 seconds)
          
          while [ $ATTEMPT -lt $MAX_ATTEMPTS ]; do
            ATTEMPT=$((ATTEMPT + 1))
            echo "Attempt $ATTEMPT/$MAX_ATTEMPTS: Checking service status..."
            
            # Get PRIMARY deployment status
            RUNNING_COUNT=$(aws ecs describe-services \
              --cluster $ECS_CLUSTER \
              --services $ECS_SERVICE \
              --query 'services[0].deployments[?status==`PRIMARY`].runningCount | [0]' \
              --output text)
            
            DESIRED_COUNT=$(aws ecs describe-services \
              --cluster $ECS_CLUSTER \
              --services $ECS_SERVICE \
              --query 'services[0].deployments[?status==`PRIMARY`].desiredCount | [0]' \
              --output text)
            
            if [ -z "$RUNNING_COUNT" ] || [ "$RUNNING_COUNT" == "None" ]; then
              RUNNING_COUNT=0
            fi
            if [ -z "$DESIRED_COUNT" ] || [ "$DESIRED_COUNT" == "None" ]; then
              DESIRED_COUNT=0
            fi
            
            echo "PRIMARY deployment: $RUNNING_COUNT/$DESIRED_COUNT tasks running"
            
            # Check if PRIMARY deployment is stable
            if [ "$RUNNING_COUNT" -eq "$DESIRED_COUNT" ] && [ "$DESIRED_COUNT" -gt 0 ]; then
              # Verify at least one task is healthy
              TASK_ARN=$(aws ecs list-tasks --cluster $ECS_CLUSTER --service-name $ECS_SERVICE --desired-status RUNNING --query 'taskArns[0]' --output text)
              if [ "$TASK_ARN" != "None" ] && [ ! -z "$TASK_ARN" ]; then
                HEALTH_STATUS=$(aws ecs describe-tasks --cluster $ECS_CLUSTER --tasks $TASK_ARN --query 'tasks[0].healthStatus' --output text)
                if [ "$HEALTH_STATUS" == "HEALTHY" ]; then
                  echo "Service stabilized successfully! PRIMARY deployment has $RUNNING_COUNT/$DESIRED_COUNT tasks running and healthy."
                  aws ecs describe-services \
                    --cluster $ECS_CLUSTER \
                    --services $ECS_SERVICE \
                    --query 'services[0].{status:status,runningCount:runningCount,desiredCount:desiredCount}' \
                    --output table
                  exit 0
                else
                  echo "Task exists but health status is: $HEALTH_STATUS"
                fi
              else
                echo "No running tasks found yet"
              fi
            fi
            
            # Check for stopped/failed tasks if no progress after a few attempts
            if [ "$RUNNING_COUNT" -eq 0 ] && [ "$DESIRED_COUNT" -gt 0 ] && [ $ATTEMPT -ge 3 ]; then
              STOPPED_TASK=$(aws ecs list-tasks --cluster $ECS_CLUSTER --service-name $ECS_SERVICE --desired-status STOPPED --query 'taskArns[0]' --output text)
              if [ "$STOPPED_TASK" != "None" ] && [ ! -z "$STOPPED_TASK" ]; then
                STOPPED_REASON=$(aws ecs describe-tasks --cluster $ECS_CLUSTER --tasks $STOPPED_TASK --query 'tasks[0].stoppedReason' --output text)
                EXIT_CODE=$(aws ecs describe-tasks --cluster $ECS_CLUSTER --tasks $STOPPED_TASK --query 'tasks[0].containers[0].exitCode' --output text)
                echo "WARNING: Found stopped task. Reason: $STOPPED_REASON, Exit Code: $EXIT_CODE"
                
                if [ "$EXIT_CODE" != "None" ] && [ "$EXIT_CODE" != "0" ]; then
                  FAILED_ATTEMPTS=$((FAILED_ATTEMPTS + 1))
                  echo "Task failed (exit code: $EXIT_CODE). Failed attempts: $FAILED_ATTEMPTS/$MAX_FAILED_ATTEMPTS"
                  
                  if [ $FAILED_ATTEMPTS -ge $MAX_FAILED_ATTEMPTS ]; then
                    echo "ERROR: Tasks are consistently failing to start. Checking logs..."
                    # Get log stream name
                    LOG_GROUP="/ecs/validator-service"
                    LOG_STREAM=$(aws logs describe-log-streams \
                      --log-group-name "$LOG_GROUP" \
                      --order-by LastEventTime \
                      --descending \
                      --max-items 1 \
                      --query 'logStreams[0].logStreamName' \
                      --output text 2>/dev/null || echo "N/A")
                    
                    if [ "$LOG_STREAM" != "None" ] && [ "$LOG_STREAM" != "N/A" ]; then
                      echo "Recent logs from $LOG_STREAM:"
                      aws logs get-log-events \
                        --log-group-name "$LOG_GROUP" \
                        --log-stream-name "$LOG_STREAM" \
                        --limit 50 \
                        --query 'events[*].message' \
                        --output text 2>/dev/null || echo "Could not retrieve logs"
                    fi
                    
                    echo "Task details:"
                    aws ecs describe-tasks \
                      --cluster $ECS_CLUSTER \
                      --tasks $STOPPED_TASK \
                      --query 'tasks[0].{lastStatus:lastStatus,stoppedReason:stoppedReason,containers:containers[*].{name:name,lastStatus:lastStatus,reason:reason,exitCode:exitCode}}' \
                      --output table
                    
                    echo "Service deployment failed: Tasks are failing to start"
                    exit 1
                  fi
                fi
              fi
            else
              # Reset failed attempts if we see progress
              if [ "$RUNNING_COUNT" -gt 0 ]; then
                FAILED_ATTEMPTS=0
              fi
            fi
            
            sleep $SLEEP_INTERVAL
          done
          
          echo "Service stabilization wait timed out after $MAX_ATTEMPTS attempts"
          echo "Checking final service status..."
          aws ecs describe-services \
            --cluster $ECS_CLUSTER \
            --services $ECS_SERVICE \
            --query 'services[0].{status:status,runningCount:runningCount,desiredCount:desiredCount,deployments:deployments[*].{id:id,status:status,runningCount:runningCount,desiredCount:desiredCount}}' \
            --output table
          echo "Checking task status..."
          TASK_ARN=$(aws ecs list-tasks --cluster $ECS_CLUSTER --service-name $ECS_SERVICE --desired-status RUNNING --query 'taskArns[0]' --output text)
          if [ "$TASK_ARN" != "None" ] && [ ! -z "$TASK_ARN" ]; then
            aws ecs describe-tasks \
              --cluster $ECS_CLUSTER \
              --tasks $TASK_ARN \
              --query 'tasks[0].{lastStatus:lastStatus,healthStatus:healthStatus,stoppedReason:stoppedReason,containers:containers[*].{name:name,lastStatus:lastStatus,reason:reason}}' \
              --output table
          fi
          exit 1

      - name: Read service URL
        id: tfout
        shell: bash
        working-directory: ${{ env.TF_DIR }}
        run: |
          set +e
          url=$(terraform output -raw validator_service_url 2>/dev/null)
          echo "validator_url=$url" >> "$GITHUB_OUTPUT"

      - name: Assert URL present
        if: ${{ steps.tfout.outputs.validator_url == '' }}
        shell: bash
        run: echo "No URL from Terraform output; skipping smoke."

      - name: Health check
        if: ${{ steps.tfout.outputs.validator_url != '' }}
        shell: bash
        run: |
          set -euo pipefail
          url="${{ steps.tfout.outputs.validator_url }}"
          for i in {1..10}; do
            code=$(curl -s -o /dev/null -w "%{http_code}" "$url/health" || true)
            echo "Attempt $i: HTTP $code"
            if [ "$code" = "200" ]; then
              echo "Health check passed"
              exit 0
            fi
            sleep 6
          done
          echo "Health check failed after 10 attempts"
          exit 1

      - name: Validate endpoint probe
        if: ${{ steps.tfout.outputs.validator_url != '' }}
        continue-on-error: true
        shell: bash
        run: |
          url="${{ steps.tfout.outputs.validator_url }}"
          echo "Probing /validate endpoint..."
          code=$(curl -s -w "%{http_code}" -X POST "$url/validate" \
            -H "Content-Type: application/json" \
            -d '{"pkgName":"test","version":"1.0.0","userId":"test","groups":["Group_106"]}' \
            -o /dev/null --max-time 10 --connect-timeout 5 || echo "000")
          echo "Validate endpoint response: HTTP $code"
          if [ "$code" = "404" ]; then
            echo "Validate endpoint responsive (404 expected for missing package)"
          elif [ "$code" = "400" ] || [ "$code" = "403" ] || [ "$code" = "500" ]; then
            echo "Validate endpoint responsive (service-level response)"
          else
            echo "Validate endpoint returned unexpected code: $code"
          fi