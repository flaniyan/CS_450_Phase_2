# ACME CLI Scoring Toolkit

ACME CLI ingests lists of model repository URLs, pulls rich metadata from GitHub and Hugging Face, evaluates programmable quality metrics, and emits newline-delimited JSON (NDJSON) summaries. The project ships with a thin runner (`run.py`/`run`) for local workflows, a library surface under `acmecli`, and a pytest suite that locks down the heuristics used to score repositories.

## Table of Contents

- [ACME CLI Scoring Toolkit](#acme-cli-scoring-toolkit)
  - [Table of Contents](#table-of-contents)
  - [Quick Start](#quick-start)
    - [Option 1: AWS Cloud Deployment (Production Ready)](#option-1-aws-cloud-deployment-production-ready)
    - [Option 2: Local Development](#option-2-local-development)
    - [Option 3: Docker (Development)](#option-3-docker-development)
  - [Package Management API](#package-management-api)
    - [üì¶ Package Operations](#-package-operations)
    - [üîß Testing and Development](#-testing-and-development)
  - [AWS Infrastructure](#aws-infrastructure)
    - [üèóÔ∏è Architecture Components](#Ô∏è-architecture-components)
    - [üöÄ CI/CD Pipeline](#-cicd-pipeline)
    - [üìä Current Deployment Status](#-current-deployment-status)
  - [Project Layout](#project-layout)
  - [CLI Entry Points](#cli-entry-points)
  - [Runtime Data Flow](#runtime-data-flow)
  - [Source Modules](#source-modules)
    - [Top-Level Runner](#top-level-runner)
    - [Handlers and Utilities](#handlers-and-utilities)
    - [Scoring and Reporting](#scoring-and-reporting)
    - [Metrics Registry](#metrics-registry)
  - [Metric Reference](#metric-reference)
  - [Testing and Tooling](#testing-and-tooling)
  - [Continuous Integration](#continuous-integration)
  - [Sample Inputs and Docs](#sample-inputs-and-docs)
  - [Extending the System](#extending-the-system)
  - [üö® AWS Deployment Troubleshooting](#-aws-deployment-troubleshooting)
    - [Common Issues and Quick Fixes](#common-issues-and-quick-fixes)
      - [502 Bad Gateway / 503 Service Temporarily Unavailable](#502-bad-gateway--503-service-temporarily-unavailable)
      - [CloudFront 403 Forbidden on Uploads](#cloudfront-403-forbidden-on-uploads)
      - [ECS Task Memory Issues](#ecs-task-memory-issues)
      - [Complete Service Reset (Nuclear Option)](#complete-service-reset-nuclear-option)
    - [Monitoring Commands](#monitoring-commands)
    - [Prevention Tips](#prevention-tips)
  - [Operational Notes and Limitations](#operational-notes-and-limitations)

## Quick Start

### Option 1: AWS Cloud Deployment (Production Ready)

The application is deployed on AWS with the following URLs:

**üåê Production URLs:**

- **Main Website**: `https://d6zjk2j65mgd4.cloudfront.net/`
- **Package Directory**: `https://d6zjk2j65mgd4.cloudfront.net/directory`
- **Upload Packages**: `https://d6zjk2j65mgd4.cloudfront.net/upload` _(Note: CloudFront POST requests need configuration fix)_
- **Rate Packages**: `https://d6zjk2j65mgd4.cloudfront.net/rate`
- **Admin Panel**: `https://d6zjk2j65mgd4.cloudfront.net/admin`

**üîß Direct AWS URLs (Bypass CloudFront):**

- **Direct Upload**: `http://validator-lb-727503296.us-east-1.elb.amazonaws.com/upload`
- **Direct Directory**: `http://validator-lb-727503296.us-east-1.elb.amazonaws.com/directory`

**üì° API Endpoints:**

- **Health Check**: `https://d6zjk2j65mgd4.cloudfront.net/health`
- **API Hello**: `https://d6zjk2j65mgd4.cloudfront.net/api/hello`
- **List Packages**: `GET https://d6zjk2j65mgd4.cloudfront.net/api/packages`
- **Upload Package**: `POST https://d6zjk2j65mgd4.cloudfront.net/api/packages/models/{model_id}/versions/{version}/upload`
- **Download Package**: `GET https://d6zjk2j65mgd4.cloudfront.net/api/packages/models/{model_id}/versions/{version}/download`
- **Reset Registry**: `POST https://d6zjk2j65mgd4.cloudfront.net/api/packages/reset`

### Option 2: Local Development

1. **Environment Setup**
   ```bash
   py -3.12 -m venv .venv
   .venv\Scripts\activate
   pip install --upgrade pip
   ```
2. **Install dependencies and project**
   ```bash
   ./run install
   ```
3. **Run the application locally**

   ```bash
   python -m src.index
   ```

   **Or using the Unix runner:**

   ```bash
   ./run install  # Install dependencies
   python -m src.index  # Run the application
   ```

4. **Access the local application**
   - Frontend home: `http://localhost:3000/`
   - Package Directory: `http://localhost:3000/directory`
   - Upload Packages: `http://localhost:3000/upload`
   - Rate Packages: `http://localhost:3000/rate`
   - Admin Panel: `http://localhost:3000/admin`
   - API health: `http://localhost:3000/health`
   - API hello: `http://localhost:3000/api/hello`

### Option 3: Docker (Development)

1. **Ensure Docker Desktop is running**
2. **Build and run with Docker Compose**
   ```bash
   docker-compose up
   ```
3. **Or build and run manually**

   ```bash
   docker build -t validator-api .
   docker run -p 3000:3000 validator-api
   ```

4. **Stop the container**
   ```bash
   docker-compose down
   ```

**Note:** When Uvicorn starts, it displays `http://0.0.0.0:3000` (listening on all interfaces). Access it at `http://localhost:3000` in your browser.

## Package Management API

The application provides a comprehensive package management system with the following capabilities:

### üì¶ Package Operations

**Upload Packages:**

- **Frontend**: Use the web interface at `/upload` to upload ZIP files containing HuggingFace models
- **API**: `POST /api/packages/models/{model_id}/versions/{version}/upload`
- **Supported formats**: ZIP files containing HuggingFace model structure (config.json, pytorch_model.bin, tokenizer files, etc.)
- **Features**: Automatic model validation, S3 storage, version management

**List Packages:**

- **Frontend**: Browse packages at `/directory` with search functionality
- **API**: `GET /api/packages?limit=100&name_regex=pattern&version_range=1.0.0-2.0.0`
- **Features**: Pagination, regex filtering, version range filtering

**Download Packages:**

- **API**: `GET /api/packages/models/{model_id}/versions/{version}/download?component=full`
- **Components**: `full` (complete model), `weights` (model weights only), `datasets` (datasets only)
- **Features**: Streaming download, component extraction

**Reset Registry:**

- **API**: `POST /api/packages/reset`
- **Purpose**: Clear all packages from the registry (admin function)

### üîß Testing and Development

**Run the test suite:**

```bash
./run test
```

Prints a coverage summary along with the pass count extracted from pytest output.

**Score repositories:**

```bash
./run score urls.txt
```

- Passing no arguments defaults to `urls.txt`.
- NDJSON is written to stdout; redirect to a file to persist results: `./run score urls.txt > reports.ndjson`.

## AWS Infrastructure

The application is deployed on AWS using the following services:

### üèóÔ∏è Architecture Components

- **ECS Fargate**: Containerized FastAPI application running on AWS managed containers
- **Application Load Balancer (ALB)**: Routes traffic to ECS tasks with health checks
- **CloudFront CDN**: Global content delivery with HTTPS and caching
- **S3 Bucket**: Package storage with Access Point for secure access
- **DynamoDB**: Metadata storage for packages and user data
- **ECR**: Container registry for Docker images
- **IAM**: Role-based access control for AWS services

### üöÄ CI/CD Pipeline

The project includes a complete CI/CD pipeline:

**Continuous Integration (CI):**

- Automated testing on pull requests
- Code quality checks and linting
- Smoke testing on main branch

**Continuous Deployment (CD):**

- Automatic Docker image build and push to ECR
- Infrastructure updates via Terraform
- ECS service deployment with health checks
- Rollback capability on deployment failures

See [CD_SETUP_GUIDE.md](CD_SETUP_GUIDE.md) for configuration instructions.

### üìä Current Deployment Status

**‚úÖ Working Features:**

- Package upload via localhost and direct AWS URLs
- Package listing and directory browsing
- Package download and streaming
- Health checks and API endpoints
- Frontend web interface

**‚ö†Ô∏è Known Issues:**

- CloudFront POST requests blocked (403 error) - needs AllowedMethods configuration update
- Use direct AWS URLs for uploads until CloudFront is fixed

**üîß Infrastructure Files:**

- `AWS_SETUP_GUIDE.md`: Complete AWS infrastructure documentation
- `AWS_IMPLEMENTATION_SUMMARY.md`: Current deployment status and issues
- `docker-compose.yml`: Local development with Docker
- `Dockerfile`: Container configuration for AWS deployment

## Project Layout

| Path                                    | Description                                                                |
| --------------------------------------- | -------------------------------------------------------------------------- |
| `run.py`                                | CLI orchestrator for install, test, and score flows.                       |
| `run`                                   | Thin executable shim that re-invokes `run.py` with the active interpreter. |
| `src/acmecli/`                          | Library package containing handlers, metrics, scoring, and types.          |
| `tests/`                                | Pytest suite that exercises metrics, scoring logic, and reporters.         |
| `frontend/templates/`                   | Jinja templates served by FastAPI for the UI.                              |
| `frontend/static/`                      | Static assets (CSS, images) served at `/static`.                           |
| `docs/`                                 | High-level documentation: overview, frontend, API/src, tests.              |
| `.github/workflows/ci.yml`              | GitHub Actions workflow mirroring the local run commands.                  |
| `urls.txt`                              | Default set of GitHub and Hugging Face URLs used for smoke scoring.        |
| `.coveragerc`, `pytest.ini`, `mypy.ini` | Tooling configuration for coverage, pytest defaults, and type checking.    |
| `autograder_docs/`                      | OpenAPI spec and sample payloads for the external autograder service.      |

## CLI Entry Points

- `python run.py install` -> installs requirements (user site outside a venv) and the project in editable mode.
- `python run.py test` -> runs pytest with coverage, summarizes pass count, and echoes logs on failure.
- `python run.py score <URL_FILE>` -> executes the scoring pipeline for comma-separated URLs per line.
- `python run.py` (no args) -> shorthand for `python run.py score urls.txt`.
- `run` script -> Unix-friendly wrapper that preserves the active interpreter when executed.

**Logging controls** (implemented in `acmecli/cli.py`):

- `LOG_FILE=/path/to/log.txt` redirects logs to disk and creates directories as needed.
- `LOG_LEVEL` accepts `0` (silent), `1` (info), or `2` (debug). The default is silent mode.

## Runtime Data Flow

1. **Entrypoint resolution** - `run.py:main` parses the command and dispatches to `do_install`, `do_test`, or `do_score`.
2. **URL ingestion** - `acmecli.cli.extract_urls` expands comma-separated URLs per line, while `classify` tags each as GitHub or Hugging Face.
3. **Metadata fetch** - `GitHubHandler.fetch_meta` and `HFHandler.fetch_meta` call the respective public APIs, normalizing selected fields and README text.
4. **Metric evaluation** - `acmecli.metrics.base.REGISTRY` holds metric instances registered via module side effects; `cli.process_url` submits each metric `score` call to a `ThreadPoolExecutor` for parallel evaluation.
5. **Aggregation** - `acmecli.scoring.compute_net_score` combines metric values with predefined weights and reports both the score and aggregation latency.
6. **Emission** - `acmecli.reporter.write_ndjson` serializes a `ReportRow` dataclass to NDJSON for downstream consumption.

## Source Modules

### Top-Level Runner

- `parse_args` (`run.py`) - builds the argparse CLI with `install`, `test`, and `score` subcommands.
- `do_install` (`run.py`) - installs `requirements.txt` (when present) followed by the package itself in editable mode.
- `do_test` (`run.py`) - runs pytest with coverage, scrapes stdout/stderr for counts, and prints a compact summary line.
- `do_score` (`run.py`) - validates the URL file and forwards execution to `acmecli.cli.main`.
- `main` (`run.py`) - wires the subcommands, logging, and the default fallback to scoring URLs.

### Handlers and Utilities

- `setup_logging` (`src/acmecli/cli.py`) - configures root logging according to `LOG_FILE` and `LOG_LEVEL` environment variables.
- `classify`, `extract_urls` (`cli.py`) - map raw URLs to internal categories and expand comma-separated URL lists.
- `GitHubHandler` (`src/acmecli/github_handler.py`) - fetches repository metadata, top contributors, and README content using the GitHub REST API.
- `HFHandler` (`src/acmecli/hf_handler.py`) - resolves Hugging Face model identifiers and retrieves model cards via the public API.
- `InMemoryCache` (`src/acmecli/cache.py`) - simple key and etag in-memory cache that can be swapped for persistent stores.
- `TargetSpec`, `MetricValue`, `ReportRow` (`src/acmecli/types.py`) - dataclasses and protocols describing targets, metric outputs, and the NDJSON schema.

### Scoring and Reporting

- `process_url` (`cli.py`) - orchestrates metadata retrieval, parallel metric execution, net score aggregation, and `ReportRow` construction.
- `compute_net_score` (`src/acmecli/scoring.py`) - multiplies metric values by weights (license 20%, ramp-up 15%, bus factor 12%, performance claims 12%, size 10%, dataset/code 10%, dataset quality 11%, code quality 10%) and averages dict-valued size scores across hardware tiers.
- `compute_netscore` (`scoring.py`) - lightweight helper used in tests to verify weighted sums.
- `write_ndjson`, `Reporter.format` (`src/acmecli/reporter.py`) - emit NDJSON rows and format arbitrary dicts as JSON strings.

### Metrics Registry

- `REGISTRY` (`src/acmecli/metrics/base.py`) - global list populated through the `register` helper.
- `acmecli/metrics/__init__.py` - imports each metric module so their `register(...)` side effects run. Import this package before using `REGISTRY` in custom integrations.

## Metric Reference

Each metric exposes `name` and `score(meta: dict) -> MetricValue`. Implementations live under `src/acmecli/metrics/` and are unit-tested in `tests/`.

| Metric                    | Module                         | Purpose and Heuristics                                                                                                                |
| ------------------------- | ------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------- |
| `LicenseMetric`           | `license_metric.py`            | Scores LGPLv2.1 compatibility based on SPDX identifiers and README cues; penalizes missing license data.                              |
| `RampUpMetric`            | `ramp_up_metric.py`            | Rewards thorough READMEs, quickstart sections, recent commits, project wikis, and star counts to reflect onboarding ease.             |
| `BusFactorMetric`         | `bus_factor_metric.py`         | Estimates sustainability using contributor counts, contribution distribution, organization ownership hints, and fork counts.          |
| `PerformanceClaimsMetric` | `performance_claims_metric.py` | Searches README text for benchmarks, metrics, comparisons, and academic references signaling evidence-backed claims.                  |
| `SizeMetric`              | `size_metric.py`               | Produces per-platform deployability scores (Raspberry Pi to AWS Server) derived from repository size and README descriptors.          |
| `DatasetAndCodeMetric`    | `dataset_and_code_metric.py`   | Checks for dataset disclosure, code pointers, examples, and project size to gauge resource availability.                              |
| `DatasetQualityMetric`    | `dataset_quality_metric.py`    | Looks for high-quality datasets, curation notes, bias considerations, and usage metrics to assess data rigor.                         |
| `CodeQualityMetric`       | `code_quality_metric.py`       | Detects testing, documentation, style, dependency management, release cadence, and issue ratios as proxies for engineering health.    |
| `HFDownloadsMetric`       | `hf_downloads_metric.py`       | Normalizes Hugging Face download counts into a 0-1 score; currently excluded from `ReportRow` aggregation but available to consumers. |
| `CLIMetric`               | `cli_metric.py`                | Rewards repositories that document CLI usage, automation scripts, or install and test instructions.                                   |
| `LoggingEnvMetric`        | `logging_env_metric.py`        | Measures logging ergonomics via environment-variable support and README mentions.                                                     |

## Testing and Tooling

- `python run.py test` executes the full pytest suite.
- Coverage configuration (`.coveragerc`) excludes network-heavy modules from coverage expectations.
- `tests/` contains focused unit tests per metric (for example `tests/test_bus_factor_metric.py`) plus checks for `Reporter` and `scoring` helpers.
- `pytest.ini` pins `tests/` as the discovery root and enforces coverage reports.
- `mypy.ini` opts into Python 3.12 typing, ignoring missing imports for third-party APIs.

## Continuous Integration

`.github/workflows/ci.yml` mirrors local workflows on a self-hosted Windows runner:

1. Checkout repository.
2. Install Python 3.12.
3. Install dependencies via `pip`.
4. Run `python3 run.py install`, `python3 run.py test`, and `python3 run.py` (smoke scoring with `urls.txt`).

## Sample Inputs and Docs

- `urls.txt` demonstrates the comma-separated URL format consumed by the scorer (GitHub repo, placeholder, Hugging Face model).
- `autograder_docs/autograder_openapi_spec.yaml` describes the external autograder API, accompanied by `sample_input.txt` and `sample_output.txt` for reference payloads.

## Extending the System

1. **Add a metric**
   - Create `src/acmecli/metrics/<new_metric>.py` with a `score` method returning `MetricValue`.
   - Call `register(NewMetric())` at module scope.
   - Import the module inside `src/acmecli/metrics/__init__.py` so it auto-registers.
   - Update `acmecli/scoring.py` weights and `ReportRow` if the metric should affect NDJSON output.
   - Add a dedicated test in `tests/`.
2. **Support another source**
   - Implement a handler following the `SourceHandler` protocol in `types.py`.
   - Update `cli.classify` and `process_url` to invoke the handler and pass its metadata to metrics.
3. **Swap caching**
   - Replace `InMemoryCache` with a persistent cache by implementing the `Cache` protocol and wiring it into `process_url`.
4. **Integrate into other programs**
   - Import `acmecli.cli`, ensure `import acmecli.metrics` runs to populate `REGISTRY`, and call `process_url` directly with custom handlers.

## üö® AWS Deployment Troubleshooting

### Common Issues and Quick Fixes

#### 502 Bad Gateway / 503 Service Temporarily Unavailable

**Symptoms:**

- CloudFront or ALB returns 502/503 errors
- Rating button fails with "502 Bad Gateway"
- Health checks fail intermittently

**Root Cause:**
ECS tasks failing to start or becoming unhealthy due to:

- Task deployment in progress
- Target group draining issues
- Memory/CPU resource constraints
- Health check failures

**Quick Fix Commands:**

```bash
# 1. Check ECS service status
aws ecs describe-services --cluster validator-cluster --services validator-service --query 'services[0].deployments[0].{Status:status,RolloutState:rolloutState,RunningCount:runningCount,DesiredCount:desiredCount,PendingCount:pendingCount,FailedTasks:failedTasks}'

# 2. Force a fresh deployment (RECOMMENDED FIX)
aws ecs update-service --cluster validator-cluster --service validator-service --force-new-deployment

# 3. Check target group health
aws elbv2 describe-target-health --target-group-arn arn:aws:elasticloadbalancing:us-east-1:838693051036:targetgroup/validator-tg-3000/69d9076ad203b619

# 4. Test endpoints after deployment
curl https://d6zjk2j65mgd4.cloudfront.net/health
curl -X POST -H "Content-Type: application/json" -d '{"target":"gpt2"}' https://d6zjk2j65mgd4.cloudfront.net/api/registry/models/gpt2/rate
```

**When to Use Each Command:**

- **Always start with step 2** (`--force-new-deployment`) - this resolves 90% of 502/503 issues
- Use step 1 to verify the fix worked
- Use step 3 if issues persist (check for draining targets)
- Use step 4 to test functionality

#### CloudFront 403 Forbidden on Uploads

**Symptoms:**

- Upload form returns "403 ERROR"
- POST requests blocked by CloudFront

**Quick Fix:**

```bash
# Update CloudFront distribution to allow POST methods
aws cloudfront get-distribution-config --id E1234567890ABCD > current-config.json
# Edit current-config.json to add POST, PUT, DELETE, OPTIONS to AllowedMethods
aws cloudfront update-distribution --id E1234567890ABCD --distribution-config file://current-config.json
```

#### ECS Task Memory Issues

**Symptoms:**

- Tasks fail to start
- "Out of memory" errors in CloudWatch logs

**Quick Fix:**

```bash
# Increase memory allocation in task definition
aws ecs describe-task-definition --task-definition validator-service:7 --query 'taskDefinition.{memory:memory,cpu:cpu}'
# Update task definition with higher memory (e.g., 1024 MB)
aws ecs update-service --cluster validator-cluster --service validator-service --task-definition validator-service:8
```

#### Complete Service Reset (Nuclear Option)

**If all else fails:**

```bash
# 1. Stop the service
aws ecs update-service --cluster validator-cluster --service validator-service --desired-count 0

# 2. Wait for tasks to stop
aws ecs wait services-stable --cluster validator-cluster --services validator-service

# 3. Start the service
aws ecs update-service --cluster validator-cluster --service validator-service --desired-count 1

# 4. Force new deployment
aws ecs update-service --cluster validator-cluster --service validator-service --force-new-deployment
```

### Monitoring Commands

```bash
# Check CloudWatch logs for errors
aws logs describe-log-streams --log-group-name "/ecs/validator-service" --order-by LastEventTime --descending --max-items 1
aws logs get-log-events --log-group-name "/ecs/validator-service" --log-stream-name "STREAM_NAME" --query 'events[-10:].message'

# Monitor ECS service events
aws ecs describe-services --cluster validator-cluster --services validator-service --query 'services[0].events[0:5]'
```

### Prevention Tips

1. **Regular Health Checks**: Monitor `/health` endpoint regularly
2. **Resource Monitoring**: Keep an eye on CloudWatch metrics for CPU/Memory usage
3. **Deployment Windows**: Schedule deployments during low-traffic periods
4. **Rollback Plan**: Always have the previous task definition ready for quick rollback

## Operational Notes and Limitations

- Network access is required for live GitHub and Hugging Face scoring; consider injecting cached metadata when offline.
- Metric evaluations rely on README heuristics and public metadata; they do not clone repositories or inspect files.
- `REGISTRY` depends on metric modules being imported; custom consumers must import `acmecli.metrics` before invoking `process_url`.
- NDJSON output currently omits `hf_downloads`, `cli`, and `logging_env` scores; extend `ReportRow` if these should be surfaced downstream.
- The cache is in-memory and per-process; restart the CLI or run in parallel to clear state.
  #   C D   P i p e l i n e   T e s t   -   1 0 / 2 2 / 2 0 2 5   1 4 : 2 1 : 2 0 
   
   
